{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c84e55b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0ebac0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from previous assignments (or based on previous assignments)\n",
    "\n",
    "class Neuron:\n",
    "    \"\"\"Class developed for HW1. Implements a single neuron and a forward pass through that neuron.\"\"\"\n",
    "    def __init__(self, weights, activation_function):\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = weights[0]\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        # Assumes that inputs are augmented to account for bias\n",
    "        return self.activation_function(np.dot(self.weights, inputs))\n",
    "\n",
    "class PerceptronTrainer:\n",
    "    \"\"\"Class developed based on submission for HW2. Implements the perceptron training algorithm.\"\"\"\n",
    "    def __init__(self, perceptron, learning_rate=0.1):\n",
    "        self.perceptron = perceptron\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def train(self, training_data, labels):\n",
    "        num_epochs = 0\n",
    "        print(f\"Epoch {num_epochs}\")\n",
    "        self.report_weights()\n",
    "        accuracy = self.report_accuracy(training_data, labels)\n",
    "        \n",
    "        while accuracy < 1.0:\n",
    "            num_epochs += 1\n",
    "            for input_x, label_d in zip(training_data, labels):\n",
    "                output_y = self.perceptron.predict(input_x)\n",
    "                self.perceptron.weights += self.learning_rate * (label_d - output_y) * input_x\n",
    "\n",
    "            print(f\"Epoch {num_epochs}\")\n",
    "            self.report_weights()\n",
    "            accuracy = self.report_accuracy(training_data, labels)\n",
    "\n",
    "    def report_weights(self):\n",
    "        print(f'\\tWeights: {self.perceptron.weights}')\n",
    "        return self.perceptron.weights\n",
    "        \n",
    "    def report_accuracy(self, training_data, labels):\n",
    "        predictions = [self.perceptron.predict(inputs) for inputs in training_data]\n",
    "        accuracy = sum(p == l for p, l in zip(predictions, labels)) / len(labels)\n",
    "        print(f'\\tAccuracy: {accuracy * 100:.2f}%')\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b0724184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnist_loader:\n",
    "    \"\"\"Class developed for HW3. Loads MNIST data from the given file path.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "            \n",
    "    def load_labels(self, filename):\n",
    "        \"\"\"MNIST labels are stored as a 1D array of bytes.\"\"\"\n",
    "        \n",
    "        with open(filename, 'rb') as f:\n",
    "            print(f\"Reading data from {filename}...\")\n",
    "            \n",
    "            magic_number, num_labels = np.frombuffer(f.read(8), dtype='>i4')\n",
    "            # print(f\"Magic number: {magic_number}, Number of labels: {num_labels}\")\n",
    "            \n",
    "            labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "            print(f\"Read {len(labels)} labels.\")\n",
    "    \n",
    "        return labels\n",
    "    \n",
    "    def load_images(self, filename):\n",
    "        \"\"\"MNIST images are stored as a 2D array of bytes, with each row representing an image.\"\"\"\n",
    "        \n",
    "        with open(filename, 'rb') as f:\n",
    "            print(f\"Reading data from {filename}...\")\n",
    "            \n",
    "            magic_number, num_images, num_rows, num_cols = np.frombuffer(f.read(16), dtype='>i4')\n",
    "            # print(f\"Magic number: {magic_number}, Number of images: {num_images}, Image size: {num_rows}x{num_cols}\")\n",
    "            \n",
    "            images = np.frombuffer(f.read(), dtype=np.uint8).reshape(num_images, num_rows * num_cols)\n",
    "            print(f\"Read {images.shape[0]} images with shape {num_rows}x{num_cols}.\")\n",
    "            \n",
    "        return images\n",
    "    \n",
    "    \n",
    "def load_training_data():\n",
    "    loader = mnist_loader()\n",
    "    train_labels = loader.load_labels('mnist/train-labels.idx1-ubyte')\n",
    "    train_images = loader.load_images('mnist/train-images.idx3-ubyte')\n",
    "\n",
    "    return train_labels, train_images\n",
    "\n",
    "# train_df = load_training_data()\n",
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dd886ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from mnist/train-labels.idx1-ubyte...\n",
      "Read 60000 labels.\n",
      "Reading data from mnist/train-images.idx3-ubyte...\n",
      "Read 60000 images with shape 28x28.\n",
      "Epoch 0\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 1\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 2\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 3\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 4\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 5\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 6\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 7\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 8\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 9\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 10\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 11\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 12\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 13\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 14\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 15\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 16\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 17\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 18\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 19\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 20\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 21\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 22\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 23\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 24\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 25\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 26\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 27\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 28\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 29\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 30\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 31\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 32\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 33\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 34\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 35\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 36\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 37\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 38\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 39\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 40\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 41\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 42\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 43\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 44\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 45\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 46\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 47\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 48\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 49\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 50\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 51\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 52\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 53\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 54\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 55\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 56\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 57\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 58\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 59\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 60\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 61\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 62\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 63\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 64\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 65\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 66\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 67\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 68\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 69\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 70\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 71\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 72\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 73\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 74\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 75\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 76\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 77\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 78\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 79\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 80\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 81\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 82\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 83\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 84\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 85\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 86\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 87\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 88\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 89\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 90\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 91\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 92\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 93\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 94\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 95\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 96\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 97\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 98\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 99\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 100\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 101\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 102\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 103\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 104\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 105\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 106\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 107\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 108\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 109\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 110\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 111\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 112\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 113\n",
      "\tAccuracy: 9.87%\n",
      "Epoch 114\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m train_labels, train_inputs = load_training_data()    \n\u001b[32m     45\u001b[39m nn = NeuralNetwork((\u001b[32m784\u001b[39m, \u001b[32m10\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mNeuralNetwork.gradient_descent\u001b[39m\u001b[34m(self, inputs_X, labels_d, learning_rate, accuracy_threshold)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Implement forward pass, backpropagation, and weight updates here\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m accuracy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcalculate_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mNeuralNetwork.calculate_accuracy\u001b[39m\u001b[34m(self, inputs_X, labels_d, report)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalculate_accuracy\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs_X, labels_d, report=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     predictions = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m inputs \u001b[38;5;129;01min\u001b[39;00m inputs_X]\n\u001b[32m     34\u001b[39m     accuracy = \u001b[38;5;28msum\u001b[39m(p == l \u001b[38;5;28;01mfor\u001b[39;00m p, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels_d)) / \u001b[38;5;28mlen\u001b[39m(labels_d)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m report:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mNeuralNetwork.predict\u001b[39m\u001b[34m(self, inputs_X)\u001b[39m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mself\u001b[39m.layers = layers\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mself\u001b[39m.weights = np.zeros(shape=\u001b[38;5;28mself\u001b[39m.layers)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs_X):\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Implement forward pass through the network\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m  \u001b[38;5;66;03m# Placeholder for predicted class\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgradient_descent\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs_X, labels_d, learning_rate=\u001b[32m0.01\u001b[39m, accuracy_threshold=\u001b[32m0.95\u001b[39m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = np.zeros(shape=self.layers)\n",
    "\n",
    "    def predict(self, inputs_X):\n",
    "        # Implement forward pass through the network\n",
    "        return 0  # Placeholder for predicted class\n",
    "                \n",
    "    def gradient_descent(self, inputs_X, labels_d, learning_rate=0.01, accuracy_threshold=0.95):\n",
    "        num_epochs = 0\n",
    "        \n",
    "        print(f\"Epoch {num_epochs}\")\n",
    "        accuracy = self.calculate_accuracy(inputs_X, labels_d, report=True)\n",
    "                \n",
    "        while accuracy < accuracy_threshold:\n",
    "            num_epochs += 1\n",
    "            # Implement forward pass, backpropagation, and weight updates here\n",
    "            \n",
    "            print(f\"Epoch {num_epochs}\")\n",
    "            accuracy = self.calculate_accuracy(inputs_X, labels_d, report=True)\n",
    "\n",
    "    def describe_network(self):\n",
    "        print(f\"Network architecture:\")\n",
    "        for layer_idx, layer in enumerate(self.layers):\n",
    "            print(f\"\\tLayer {layer_idx + 1}: {layer} neurons\")\n",
    "        \n",
    "    def calculate_loss(self, inputs_X, labels_d, report=False):\n",
    "        # Implement loss calculation (e.g., cross-entropy loss)\n",
    "        return 0.0  # Placeholder for loss value\n",
    "    \n",
    "    def calculate_accuracy(self, inputs_X, labels_d, report=False):\n",
    "        predictions = [self.predict(inputs) for inputs in inputs_X]\n",
    "        accuracy = sum(p == l for p, l in zip(predictions, labels_d)) / len(labels_d)\n",
    "        \n",
    "        if report:\n",
    "            print(f'\\tAccuracy: {accuracy * 100:.2f}%')\n",
    "        \n",
    "        return accuracy\n",
    "        \n",
    "        \n",
    "\n",
    "train_labels, train_inputs = load_training_data()    \n",
    "\n",
    "nn = NeuralNetwork((784, 10))\n",
    "nn.gradient_descent(train_inputs, train_labels, learning_rate=0.01, accuracy_threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6644057e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
